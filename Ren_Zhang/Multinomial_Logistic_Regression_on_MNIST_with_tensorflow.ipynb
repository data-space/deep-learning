{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "from __future__ import division\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import gc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4.16 s, sys: 224 ms, total: 4.39 s\n",
      "Wall time: 6.46 s\n",
      "(22512, 784) (11088, 784) (8400, 784)\n"
     ]
    }
   ],
   "source": [
    "# read in data\n",
    "%time data = pd.read_csv(\"./data/train.csv\")\n",
    "\n",
    "# seperate out label\n",
    "y = data[['label']]\n",
    "X = data.drop('label', axis = 1).astype(np.float32).values / 255   # tensorflow prefer 32 bit floating point numbers\n",
    "\n",
    "# split data into train, validation and test\n",
    "\n",
    "X, X_test, y, y_test = train_test_split(X, y, test_size=0.20, stratify = y, random_state = 36)\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.33, stratify = y, random_state = 2014)\n",
    "\n",
    "del [X, y]\n",
    "gc.collect()\n",
    "\n",
    "print(X_train.shape, X_valid.shape, X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   test_labels  train_labels  valid_lables\n",
      "1          937          2510          1237\n",
      "7          880          2359          1162\n",
      "3          870          2332          1149\n",
      "9          838          2245          1105\n",
      "2          835          2239          1103\n",
      "6          827          2218          1092\n",
      "0          826          2215          1091\n",
      "4          815          2182          1075\n",
      "8          813          2178          1072\n",
      "5          759          2034          1002\n"
     ]
    }
   ],
   "source": [
    "# check the label balance\n",
    "print(pd.DataFrame({\n",
    "            'train_labels': y_train.label.value_counts(),\n",
    "            'valid_lables': y_valid.label.value_counts(),\n",
    "            'test_labels': y_test.label.value_counts()\n",
    "        }))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(22512, 10) (11088, 10) (8400, 10)\n"
     ]
    }
   ],
   "source": [
    "# use one-hot encoding to encode label\n",
    "le = OneHotEncoder()\n",
    "le.fit(y_train[['label']])\n",
    "\n",
    "y_train = le.transform(y_train).astype(np.float32).toarray()\n",
    "y_valid = le.transform(y_valid).astype(np.float32).toarray()\n",
    "y_test = le.transform(y_test).astype(np.float32).toarray()\n",
    "\n",
    "print(y_train.shape, y_valid.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.  0.  0.  0.  0.  0.  1.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  1.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  1.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  1.]\n",
      " [ 0.  0.  1.  0.  0.  0.  0.  0.  0.  0.]]\n"
     ]
    }
   ],
   "source": [
    "# see the encoded responses variables\n",
    "print(y_train[:5, :])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# construct the computation graph with tensorflow\n",
    "\n",
    "graph = tf.Graph()\n",
    "\n",
    "with graph.as_default():\n",
    "    # specify the dataset used in computation, for now just pass them as constants\n",
    "    train_dataset = tf.constant(X_train)\n",
    "    train_labels = tf.constant(y_train)\n",
    "    valid_dataset = tf.constant(X_valid)\n",
    "    test_dataset = tf.constant(X_test)\n",
    "\n",
    "    # setup how weights are initialized: using a truncated normal distribution\n",
    "    W = tf.Variable(tf.truncated_normal([784, 10]))\n",
    "    # setup how biases are initialized: all set to zero.\n",
    "    b = tf.Variable(tf.zeros([10]))\n",
    "    \n",
    "    # in the graph there is just one node and all it does is a linear mapping from inputs(784) to logits(10) \n",
    "    # logits = Wx + b\n",
    "    logits = tf.matmul(train_dataset, W) + b\n",
    "    \n",
    "    # specifiy the loss used in training, here I am using log-loss\n",
    "    # loss function is where the gradients are calculated (based on the back-propagated error)\n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits, train_labels))\n",
    "    \n",
    "    # specify an optimizer to use to update W and b, here i am using 0.5 as the learning rate\n",
    "    optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)   \n",
    "    \n",
    "    # use softmax to convert logits into proper probabilities(sum to 1 across each row) \n",
    "    train_prediction = tf.nn.softmax(logits)\n",
    "    valid_prediction = tf.nn.softmax(tf.matmul(valid_dataset, W) + b)\n",
    "    test_prediction = tf.nn.softmax(tf.matmul(test_dataset, W) + b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation Metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def accuracy(predictions, labels):\n",
    "  return (100.0 * np.sum(np.argmax(predictions, 1) == np.argmax(labels, 1)) / predictions.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss at step 0: 12.880156\n",
      "Training accuracy: 6.3%\n",
      "Validation accuracy: 6.9%\n",
      "Test accuracy: 6.7%\n",
      "========================================================================\n",
      "Loss at step 100: 1.130349\n",
      "Training accuracy: 75.2%\n",
      "Validation accuracy: 75.2%\n",
      "Test accuracy: 74.5%\n",
      "========================================================================\n",
      "Loss at step 200: 0.812949\n",
      "Training accuracy: 81.4%\n",
      "Validation accuracy: 81.3%\n",
      "Test accuracy: 80.5%\n",
      "========================================================================\n",
      "Loss at step 300: 0.685550\n",
      "Training accuracy: 83.8%\n",
      "Validation accuracy: 83.5%\n",
      "Test accuracy: 82.7%\n",
      "========================================================================\n",
      "Loss at step 400: 0.611366\n",
      "Training accuracy: 85.1%\n",
      "Validation accuracy: 84.8%\n",
      "Test accuracy: 84.3%\n",
      "========================================================================\n"
     ]
    }
   ],
   "source": [
    "num_steps = 500 \n",
    "\n",
    "# start a computation session\n",
    "with tf.Session(graph=graph) as session:\n",
    "    # initialize weights and bias\n",
    "    tf.initialize_all_variables().run()\n",
    "    \n",
    "    # \n",
    "    for step in range(num_steps):\n",
    "        _, l, predictions = session.run([optimizer, loss, train_prediction])\n",
    "        if (step % 100 == 0):\n",
    "            print('Loss at step %d: %f' % (step, l))\n",
    "            print('Training accuracy: %.1f%%' % accuracy(predictions, y_train))\n",
    "            print('Validation accuracy: %.1f%%' % accuracy(valid_prediction.eval(), y_valid))\n",
    "            print('Test accuracy: %.1f%%' % accuracy(test_prediction.eval(), y_test))\n",
    "            print(\"========================================================================\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What's Next?\n",
    "\n",
    "For the next time, we are going to use hidden layers. "
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow]",
   "language": "python",
   "name": "conda-env-tensorflow-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
